{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge distillation\n",
    "\n",
    "## Purpose\n",
    "This notebook is developed to test knowledge distillation from different MobileNet versions to Inception FaceNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Libary import\n",
    "We import all the required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "sns.set_theme(context='notebook', style='darkgrid', palette='pastel')\n",
    "\n",
    "# General\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository import\n",
    "\n",
    "For these experiments, we need to import FaceNet from [timesler/facenet-pytorch](https://github.com/timesler/facenet-pytorch): we will use it as the teacher network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/timesler/facenet-pytorch facenet_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preliminary tests, we will focus on using the MobileNet implmementation present in the `torchvision.models`. In the future, we will most likely import other repositories to test knowledge distilattion on different editions of MobileNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter definition\n",
    "\n",
    "We set all the relevant parameters for our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on device: cuda:0\nBatch size: 16\n"
     ]
    }
   ],
   "source": [
    "## General\n",
    "# Set the device to a GPU if available.\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Running on device: {DEVICE}')\n",
    "# Set the batch size.\n",
    "BATCH_SIZE = 16\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "\n",
    "## InceptionResnetV1\n",
    "# Defines whether to load a model pre-trained on `vggface2` or on\n",
    "# `casia-webface`. The first one performs a little better (0.9965 on LFW\n",
    "# compared to 0.9905) and is slightly smaller (107MB compared to 111MB).\n",
    "PRETRAINED = \"vggface2\"\n",
    "\n",
    "## MTCNN\n",
    "# Define the output image size and margin of the MTCNN module.\n",
    "IMAGE_SIZE = 160\n",
    "MARGIN = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Data import\n",
    "To train the student, we need to work on the same training set as of the teacher. Therefore, we retrieve the VGGFace2 dataset.\n",
    "\n",
    "**Note**: to retrieve the VGGFace2 dataset, you need to have an account for [Zeus Robots](http://zeus.robots.ox.ac.uk/vgg_face2/).\n",
    "\n",
    "Download all of the four files available in the page of the dataset. The two archives contains the train and the test dataset, while the two `.txt` files contains the list of paths to the train and test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "To visualize the data, we write some methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(tensor, denormalize=False):\n",
    "    \"\"\"Print a Tensor as an image.\"\"\"\n",
    "    \n",
    "    if denormalize:\n",
    "      transforms = T.Compose([T.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                          std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                              T.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                          std = [ 1., 1., 1. ]),\n",
    "                              T.ToPILImage()])\n",
    "    else:\n",
    "      transforms = T.Compose([T.ToPILImage()])\n",
    "\n",
    "    img = transforms(tensor)\n",
    "\n",
    "    return img\n",
    "\n",
    "def show_images(tensors):\n",
    "  \"\"\"\n",
    "  Print a list of Tensors as images.\n",
    "  \n",
    "    Note: if a sample from a TripletDataset is passed, it will manually\n",
    "        extract the images.\n",
    "  \"\"\"\n",
    "\n",
    "  if isinstance(tensors, dict) and 'anchor_image' in tensors \\\n",
    "      and 'positive_image' in tensors and 'negative_image' in tensors:\n",
    "      tensors = [tensors['anchor_image'], tensors['positive_image'], tensors['negative_image']]\n",
    "\n",
    "  size = math.ceil(math.sqrt(len(tensors)))\n",
    "  stop = len(tensors) + 1\n",
    "\n",
    "  # Create a new figure.\n",
    "  fig = plt.figure(figsize=(size*4, size*4))\n",
    "\n",
    "  # Set the horizontal space between subplots.\n",
    "  plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "  for i in range(1, stop):\n",
    "    # Get the tensor as an image.\n",
    "    image = show_image(tensors[i-1])\n",
    "\n",
    "    # Add the image as a subplot.\n",
    "    fig.add_subplot(size, size, i)\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Hide axis.\n",
    "    plt.axis('off')\n",
    "  \n",
    "  # Show the plot.\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data process\n",
    "\n",
    "Images must be aligned to be passed as inputs to the model. Therefore, we define a function that takes either a single tensor (representing an image) or a list of tensors and return the ones that is able to align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_images(tensors):\n",
    "    \"\"\"Get a list of images and align them.\"\"\"\n",
    "    if not isinstance(tensors, list):\n",
    "        tensors = [tensors]\n",
    "    \n",
    "    aligned_tensors = []\n",
    "    for tensor in tensors:\n",
    "        try:\n",
    "            aligned_tensors.append(mtcnn(tensor))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if len(aligned_tensors) == 1:\n",
    "        return aligned_tensors[0]\n",
    "    \n",
    "    return aligned_tensors\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset definition\n",
    "Below, we define a Dataset class for the VGGFace2 dataset. The following is a complex implementation based on triplets."
   ]
  },
  {
   "source": [
    "**NOTE**: multiprocessing does not work in interactive mode. Therefore, to generate the triplet dataset, you need to copy everything that is needed to a file and call it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    \"\"\"Dataset composed of triplets.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, split, list_path, num_triplets=None,\n",
    "                 num_processes=0, triplets_path=None):\n",
    "        \"\"\"\n",
    "            Arguments:\n",
    "                root_dir (string): root directory of the images.\n",
    "                split (string): either train or test.\n",
    "                list_path (string): path to the file containing all the names of the images.\n",
    "                num_triplets (int): number of triplets to generate.\n",
    "                num_processes (int): number of processes to use to generate triplets.\n",
    "                triplets_path (string): string to the numpy file containing the triplets.\n",
    "                    Instead of generating a new list, it's possible to pass a previously\n",
    "                    generated one.\n",
    "        \"\"\"\n",
    "        if num_triplets == triplets_path == None:\n",
    "            raise AttributeError('num_triplets and triplets_path cannot both be None.')\n",
    "\n",
    "        self.df = self._extract_dataframe(list_path)\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self._mtcnn = MTCNN(image_size=IMAGE_SIZE, margin=MARGIN, device=DEVICE)\n",
    "\n",
    "        if num_processes == 0:\n",
    "            num_processes = os.cpu_count()\n",
    "\n",
    "        if triplets_path == None:\n",
    "            self.triplets = self.generate_triplets(num_triplets,  num_processes)\n",
    "        else:\n",
    "            self.triplets = np.load(triplets_path, allow_pickle=True)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_path, positive_path, negative_path, positive_class, negative_class = self.triplets[idx]\n",
    "\n",
    "        # Open the images.\n",
    "        anchor, positive, negative = self._get_aligned_images([anchor_path, positive_path, negative_path])\n",
    "\n",
    "        sample = {\n",
    "            'anchor_image': anchor,\n",
    "            'positive_image': positive,\n",
    "            'negative_image': negative,\n",
    "            'positive_class': positive_class,\n",
    "            'negative_class': negative_class\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "\n",
    "    def _get_aligned_images(self, paths):\n",
    "        \"\"\"\n",
    "        Get a list of paths to images of the dataset and returns their aligned tensor.\n",
    "        If the aligned version is not existant, it creates it.\n",
    "        \"\"\"\n",
    "        if not isinstance(paths, list):\n",
    "            paths = [paths]\n",
    "              \n",
    "        \n",
    "        aligned_tensors = []\n",
    "        for path in paths:\n",
    "            aligned_path = os.path.join(self.root_dir, self.split + '_aligned', path)\n",
    "            normal_path = os.path.join(self.root_dir, self.split, path)\n",
    "\n",
    "            if os.path.exists(aligned_path):\n",
    "                tensor = T.ToTensor()(Image.open(aligned_path))\n",
    "            else:\n",
    "                img = Image.open(normal_path)\n",
    "                try:\n",
    "                    tensor = self._mtcnn(img, save_path=aligned_path)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if self.split == 'train':\n",
    "                tensor = T.RandomHorizontalFlip()(tensor)\n",
    "\n",
    "            aligned_tensors.append(tensor)\n",
    "        \n",
    "        if len(aligned_tensors) == 1:\n",
    "            return aligned_tensors[0]\n",
    "        \n",
    "        return aligned_tensors\n",
    "\n",
    "    \n",
    "    def _get_image_annotations(self, image_path):\n",
    "        \"\"\"Split the pathname to an image into its ids.\"\"\"\n",
    "        class_id, other = image_path.split('/')\n",
    "        image_id, other = other.split('_')\n",
    "        face_id, _ = other.split('.')\n",
    "        \n",
    "        return [class_id, image_id, face_id, image_path]\n",
    "\n",
    "\n",
    "    def _extract_dataframe(self, list_path):\n",
    "        \"\"\"Given the file containing the paths to the images, split to get the ids.\"\"\"\n",
    "        images = [line.strip() for line in open(list_path).readlines()]\n",
    "        annotations = [self._get_image_annotations(image) for image in images]\n",
    "\n",
    "        return pd.DataFrame(data=annotations, columns=['class_id', 'image_id', 'face_id', 'path'])\n",
    "\n",
    "\n",
    "    def _generate_triplets(self, num_triplets, process_id):\n",
    "        classes = self.df['class_id'].unique()\n",
    "        triplets = []\n",
    "\n",
    "        progress_bar = tqdm(range(int(num_triplets)))\n",
    "\n",
    "        for _ in progress_bar:\n",
    "            # Select a positive and a negative class.\n",
    "            pos_class = random.choice(classes)\n",
    "            neg_class = random.choice(classes)\n",
    "            \n",
    "            # Ensure that the positive class has at least two images.\n",
    "            while len(self.df.loc[self.df['class_id'] == pos_class]) < 2:\n",
    "                pos_class = random.choice(classes)\n",
    "\n",
    "            # Ensure that the classes are different.\n",
    "            while neg_class == pos_class:\n",
    "                neg_class = random.choice(classes)\n",
    "            \n",
    "            # Get the list of images of the classes.\n",
    "            pos_images = self.df.loc[self.df['class_id'] == pos_class].values\n",
    "            neg_images = self.df.loc[self.df['class_id'] == neg_class].values\n",
    "\n",
    "            # Select an anchor and a positive image.\n",
    "            anchor_index = random.randint(0, len(pos_images)-1)\n",
    "            positive_index = random.randint(0, len(pos_images)-1)\n",
    "\n",
    "            # Ensure that the positive image is not equal to the anchor.\n",
    "            while anchor_index == positive_index:\n",
    "                positive_index = random.randint(0, len(pos_images)-1)\n",
    "            \n",
    "            # Select a negative image.\n",
    "            negative = random.choice(neg_images)\n",
    "\n",
    "            # Retrieve the paths to the images\n",
    "            anchor_path = pos_images[anchor_index][-1]\n",
    "            positive_path = pos_images[positive_index][-1]\n",
    "            negative_path = negative[-1]\n",
    "\n",
    "            # Align the three images.\n",
    "            aligned_images = self._get_aligned_images([anchor_path, positive_path, negative_path])\n",
    "\n",
    "            # If the number of aligned images is 3, MTCNN was able to align all of them,\n",
    "            # therefore we save the triplet.\n",
    "            if len(aligned_images) == 3:\n",
    "                triplet = [anchor_path,\n",
    "                        positive_path,\n",
    "                        negative_path,\n",
    "                        pos_class,\n",
    "                        neg_class]\n",
    "                triplets.append(triplet)\n",
    "        \n",
    "        # Update the number of triplets.\n",
    "        self.num_triplets = len(triplets)\n",
    "\n",
    "        # Save the list to file.\n",
    "        path = os.path.join(self.root_dir, 'temp', f'{split}_triplets_{process_id}.npy')\n",
    "        np.save(path, triplets)\n",
    "\n",
    "        return triplets\n",
    "\n",
    "\n",
    "    def generate_triplets(self, num_triplets, num_processes):\n",
    "        # NOTE: the code is taken from https://github.com/tamerthamoqa/facenet-pytorch-vggface2\n",
    "        total_triplets = []\n",
    "        \n",
    "        print(\"\\nGenerating {} triplets using {} Python processes ...\".format(\n",
    "                num_triplets,\n",
    "                num_processes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # If True, there are residual number of triplets to be generated after the processes are done\n",
    "        flag_residual_triplets = False\n",
    "        triplet_residual = num_triplets % num_processes\n",
    "\n",
    "        if triplet_residual == 0:\n",
    "            num_triplets_per_process = num_triplets / num_processes\n",
    "        else:\n",
    "            flag_residual_triplets = True\n",
    "            num_triplets_per_process = num_triplets - triplet_residual\n",
    "            num_triplets_per_process = num_triplets_per_process / num_processes\n",
    "        \n",
    "        processes = []\n",
    "        for i in range(num_processes):\n",
    "            processes.append(multiprocessing.Process(\n",
    "                    target=self._generate_triplets,\n",
    "                    args=(num_triplets_per_process, i)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        for process in processes:\n",
    "            process.start()\n",
    "        \n",
    "        for process in processes:\n",
    "            process.join()  # Block execution until all spawned processes are done\n",
    "        \n",
    "        if flag_residual_triplets:\n",
    "            print(\"Processes are done. Residual number of tripelts {} detected\" + \\\n",
    "                  \"and are being generated by main process ...\".format(triplet_residual))\n",
    "            self._generate_triplets(\n",
    "                num_triplets=triplet_residual,\n",
    "                process_id=num_processes + 1\n",
    "            )\n",
    "        \n",
    "        numpy_files = glob(os.path.join(self.root_dir, 'temp', '*.npy'))\n",
    "        for numpy_file in numpy_files:\n",
    "            total_triplets.append(np.load(numpy_file))\n",
    "            os.remove(numpy_file)\n",
    "        \n",
    "        # Convert total triplets list from 3D shape to 2D shape\n",
    "        total_triplets = [elem for list in total_triplets for elem in list]\n",
    "\n",
    "        # Update the triplets and the num_triplets of the dataset.\n",
    "        self.triplets = total_triplets\n",
    "        self.num_triplets = len(total_triplets)\n",
    "\n",
    "        print(\"Saving training triplets list...\")\n",
    "        np.save(os.path.join(self.root_dir, f'{self.split}_triplets_{self.num_triplets}.npy'), total_triplets)\n",
    "        print(\"Saved!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate the train and the test set with their loaders. For the first execution of TripletDataset for the train and the test set, it's better to define the num_triplets in the method. By doing so, the Dataset will generate the specified number of triplets. For the next executions, this step can be omitted and we can instead pass the triplets_path argument to load the previously created list of triplets for the dataset.\n",
    "\n",
    "The test set has 169'396 images, while the training set has 3'141'890. For the time being, we create a train set with 10000 triplets and a test set with 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the datasets.\n",
    "train_set = TripletDataset('VGGFace2', 'train', 'VGGFace2/train_list.txt', triplets_path='VGGFace2/train_triplets_9968.npy')\n",
    "test_set = TripletDataset('VGGFace2', 'test', 'VGGFace2/test_list.txt', triplets_path='VGGFace2/test_triplets_998.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Teacher initialization\n",
    "We instantiate the teacher model, as described in its repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "# Create an inception resnet (in eval mode).\n",
    "teacher = InceptionResnetV1(pretrained='vggface2', classify=True).eval().to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student initialization\n",
    "For the time being, we resort to use the MobileNet defined in the `torchvision` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "# Create a mobilenet model.\n",
    "student = mobilenet_v2(pretrained=False).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the last layer of the student and the teacher differ, we must overwrite the `classifier` module of the student to make it similar to the one of the teacher.\n",
    "\n",
    "Note that the facenet model we are using right now, has two different evaluation modes:\n",
    "\n",
    " 1. `classify=True`: returns the logits of the classes (8631 outputs features).\n",
    " 2. `classify=False`: returns the embeddings (512 outputs features).\n",
    " \n",
    "Above, we had set the teacher to `classify=True`, therefore we add the third and fourth module in the overwritten classifier layer of the student to make the output of the student equals in size the number of classes of the teacher training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.classifier = nn.Sequential(\n",
    "    # Keep the same dropout as of the base mobilenet.\n",
    "    nn.Dropout(p=0.2, inplace=False),\n",
    "\n",
    "    # Add a linear that matches the out_features of the teacher.\n",
    "    nn.Linear(in_features=1280, out_features=512, bias=False).to(DEVICE),\n",
    "\n",
    "    # Add a batch norm as in the teacher.\n",
    "    nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True).to(DEVICE),\n",
    "\n",
    "    # Add a last linear that matches the number of classes in VGGFace2.\n",
    "    nn.Linear(512, 8631).to(DEVICE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student training\n",
    "\n",
    "Below, we define a function to be used to perform a single training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(student, teacher, classes, loader, temperature, optimizer, scheduler, epoch,\n",
    "              device=DEVICE, print_every=100):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_soft_loss = 0.0\n",
    "    running_hard_loss = 0.0\n",
    "\n",
    "    count = 0\n",
    "    for _, sample in enumerate(loader):\n",
    "        # Reset the losses.\n",
    "        soft_loss = 0\n",
    "        hard_loss = 0\n",
    "\n",
    "        # Split the sample into images and labels.\n",
    "        values = list(sample.values())\n",
    "        sample_size = len(values[0])\n",
    "        inputs = torch.cat(values[:3], 0).to(DEVICE)\n",
    "        labels = [values[3], *values[3:]]\n",
    "\n",
    "        # Convert the class names with the class id and transpose the tensor.\n",
    "        labels = [[classes.index(l) for l in label] for label in labels]\n",
    "        labels = torch.LongTensor(labels).T.to(DEVICE)\n",
    "\n",
    "        # Images and labels of the same triplet can be retrieved as follow:\n",
    "        # labels[ID], inputs[ID::SAMPLE_SIZE]\n",
    "        # Note: to access the inputs you should use the current sample size\n",
    "        # and not the general batch size.\n",
    "\n",
    "        # Zero the parameter gradient.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward.\n",
    "        student_outputs = student(inputs)\n",
    "        teacher_outputs = teacher(inputs)\n",
    "\n",
    "        # Evaluate the soft loss.\n",
    "        for i in range(sample_size):\n",
    "            soft_outputs = F.log_softmax(student_outputs[i::sample_size] / temperature, dim=1)\n",
    "            soft_targets = F.softmax(teacher_outputs[i::sample_size] / temperature, dim=1)\n",
    "            soft_loss += F.kl_div(soft_outputs, soft_targets.detach(), reduction='batchmean')\n",
    "\n",
    "        # It is important to multiply the soft loss by T^2 when using both hard and soft\n",
    "        # targets. This ensures that the relative contributions of the hard and soft\n",
    "        # targets remain roughly unchanged if the temperature used for distillation is\n",
    "        # changed while experimenting with meta-parameters.\n",
    "        soft_loss *= (temperature ** 2)\n",
    "\n",
    "        # Evaluate the hard loss.\n",
    "        for i in range(sample_size):\n",
    "            hard_loss += F.cross_entropy(student_outputs[i::sample_size], labels[i])\n",
    "\n",
    "        # Evaluate the weighted average loss.\n",
    "        loss = soft_loss + hard_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item() / sample_size\n",
    "        running_soft_loss += soft_loss.item() / sample_size\n",
    "        running_hard_loss += hard_loss.item() / sample_size\n",
    "        if count % print_every == print_every - 1:\n",
    "            print('[%d, %5d] loss: %.3f (soft: %.3f, hard: %.3f)' % \\\n",
    "                    (epoch + 1, count + 1,\n",
    "                    running_loss / print_every,\n",
    "                    running_soft_loss / print_every,\n",
    "                    running_hard_loss / print_every))\n",
    "            running_loss = 0.0\n",
    "            running_soft_loss = 0.0\n",
    "            running_hard_loss = 0.0\n",
    "        \n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student testing\n",
    "Here, we define a function to test the results reached by the model. A simple solution to test the quality of the student is to evaluate triplets of images and check the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(student, teacher, classes, loader, device=DEVICE):\n",
    "    student.eval()\n",
    "    teacher.eval()\n",
    "\n",
    "    teacher_class_accuracy = 0\n",
    "    teacher_match_accuracy = 0\n",
    "    student_class_accuracy = 0\n",
    "    student_match_accuracy = 0\n",
    "\n",
    "    for _, sample in enumerate(loader):\n",
    "        # Split the sample into images and labels.\n",
    "        values = list(sample.values())\n",
    "        sample_size = len(values[0])\n",
    "        inputs = torch.cat(values[:3], 0).to(DEVICE)\n",
    "        # labels = [values[3], *values[3:]]\n",
    "\n",
    "        # # Convert the class names with the class id and transpose the tensor.\n",
    "        # labels = [[classes.index(l) for l in label] for label in labels]\n",
    "        # labels = torch.LongTensor(labels).T.to(DEVICE)\n",
    "\n",
    "        # Forward.\n",
    "        student_outputs = student(inputs)\n",
    "        teacher_outputs = teacher(inputs)\n",
    "\n",
    "        # # Evaluate class predictions.\n",
    "        # student_class_predictions = [classes[int(student_outputs.topk(1).indices)] for output in student_outputs]\n",
    "        # teacher_class_predictions = [classes[int(teacher_outputs.topk(1).indices)] for output in teacher_outputs]\n",
    "\n",
    "        # # Update class accuracies.\n",
    "        # for i in range(3):\n",
    "        #     if student_class_predictions[i] == labels[i]:\n",
    "        #         student_class_accuracy += 1\n",
    "        #     if teacher_class_predictions[i] == labels[i]:\n",
    "        #         teacher_class_accuracy += 1\n",
    "        \n",
    "        # Evaluate match predictions for each triplet.\n",
    "        for i in range(sample_size):\n",
    "            out = student_outputs[i::sample_size]\n",
    "            student_match_distance = [(out[0] - out[1]).norm().item(),\n",
    "                                      (out[0] - out[2]).norm().item()]\n",
    "\n",
    "            out = teacher_outputs[i::sample_size]\n",
    "            teacher_match_distance = [(out[0] - out[1]).norm().item(),\n",
    "                                      (out[0] - out[2]).norm().item()]\n",
    "        \n",
    "            # Update match accuracy.\n",
    "            if student_match_distance[0] < student_match_distance[1]:\n",
    "                student_match_accuracy += 1\n",
    "            \n",
    "            if teacher_match_distance[0] < teacher_match_distance[1]:\n",
    "                teacher_match_accuracy += 1\n",
    "    \n",
    "    print(f' - Student match accuracy: {student_match_accuracy/len(loader.dataset)}')\n",
    "    # print(f'  - class: {student_class_accuracy/(3*len(loader.dataset))}')\n",
    "    # print(f'  - match: {student_match_accuracy/len(loader.dataset)}')\n",
    "    print(f' - Teacher match accuracy: {teacher_match_accuracy/len(loader.dataset)}')\n",
    "    # print(f'  - class: {teacher_class_accuracy/(3*len(loader.dataset))}')\n",
    "    # print(f'  - match: {teacher_match_accuracy/len(loader.dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "Finally, we write the function that, given a student and a teacher, perform knowledge distilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill(student, teacher, epochs, initial_temperature):\n",
    "    # Define the list of classes in the database.\n",
    "    classes = [folder[-7:] for folder in glob(os.path.join('VGGFace2', 'train', '*'))]\n",
    "\n",
    "    # Create the loader dictionary.\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_set, batch_size=BATCH_SIZE, num_workers=0),\n",
    "        'test': DataLoader(test_set, batch_size=BATCH_SIZE, num_workers=0)\n",
    "    }\n",
    "\n",
    "    # Set teacher to evaluation mode.\n",
    "    teacher.eval()\n",
    "\n",
    "    # Define the different temperatures.\n",
    "    temperatures = np.linspace(initial_temperature, 1.0, epochs).tolist()\n",
    "\n",
    "    # Instantiate optimizer and scheduler.\n",
    "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [5, 10])\n",
    "\n",
    "    # Run the epochs.\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch+1}, distillation temperature = {temperatures[epoch]}')\n",
    "\n",
    "        # Run a train step.\n",
    "        print('Training:')\n",
    "        train_step(student, teacher, classes, loaders['train'], temperatures[epoch], optimizer,\n",
    "                   scheduler, epoch)\n",
    "\n",
    "        # Run a test step.\n",
    "        print('Testing:')\n",
    "        test_step(student, teacher, classes, loaders['test'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distill(student, teacher, 20, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-66e8c395",
   "language": "python",
   "display_name": "PyCharm (face-recognition)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}